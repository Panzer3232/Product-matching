"""
Complete Environment Test
Tests all components needed for the product matching system
"""

import torch
import sys
import warnings
warnings.filterwarnings("ignore")

def test_basic_setup():
    print("=== BASIC SETUP TEST ===")
    print(f"Python Version: {sys.version.split()[0]}")
    print(f"PyTorch Version: {torch.__version__}")
    print(f"CUDA Available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"GPU: {gpu_name} ({gpu_memory:.1f} GB)")
        print("‚úÖ Basic setup OK")
    else:
        print("‚ùå CUDA not available")
    print()

def test_core_libraries():
    print("=== CORE LIBRARIES TEST ===")
    
    libraries = [
        ("transformers", "transformers"),
        ("sentence_transformers", "sentence_transformers"), 
        ("PIL", "PIL"),
        ("opencv", "cv2"),
        ("numpy", "numpy"),
        ("clip", "clip"),
        ("faiss", "faiss"),
        ("pymongo", "pymongo"),
        ("fastapi", "fastapi")
    ]
    
    results = {}
    for name, import_name in libraries:
        try:
            __import__(import_name)
            print(f"‚úÖ {name}: OK")
            results[name] = True
        except ImportError as e:
            print(f"‚ùå {name}: {str(e)[:60]}...")
            results[name] = False
    
    return results

def test_sentence_transformers_detailed():
    print("\n=== SENTENCE TRANSFORMERS DETAILED TEST ===")
    
    try:
        from sentence_transformers import SentenceTransformer
        
        # Try loading a small model
        model_name = "all-MiniLM-L6-v2"
        print(f"Loading model: {model_name}")
        
        if torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
            
        model = SentenceTransformer(model_name, device=device)
        print(f"‚úÖ Model loaded successfully on {device}")
        
        # Test encoding
        test_text = "This is a test sentence."
        embedding = model.encode(test_text)
        print(f"‚úÖ Text encoding successful. Embedding shape: {embedding.shape}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Sentence transformers detailed test failed: {e}")
        return False

def test_tensorrt():
    print("\n=== TENSORRT TEST ===")
    
    try:
        import tensorrt as trt
        print(f"‚úÖ TensorRT imported successfully")
        print(f"TensorRT Version: {trt.__version__}")
        
        # Test builder
        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
        builder = trt.Builder(TRT_LOGGER)
        print(f"‚úÖ TensorRT Builder created")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå TensorRT not installed: {e}")
        return False
    except Exception as e:
        print(f"‚ùå TensorRT error: {e}")
        return False

def test_triton():
    print("\n=== TRITON CLIENT TEST ===")
    
    try:
        import tritonclient.http as httpclient
        print(f"‚úÖ Triton HTTP client OK")
        
        import tritonclient.grpc as grpcclient
        print(f"‚úÖ Triton gRPC client OK")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Triton client not installed: {e}")
        return False

def test_clip_detailed():
    print("\n=== CLIP DETAILED TEST ===")
    
    try:
        import clip
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model, preprocess = clip.load("ViT-B/32", device=device)
        
        print(f"‚úÖ CLIP ViT-B/32 loaded on {device}")
        
        # Test with dummy image
        dummy_image = torch.randn(1, 3, 224, 224).to(device)
        
        with torch.no_grad():
            image_features = model.encode_image(dummy_image)
            
        print(f"‚úÖ Image encoding successful. Shape: {image_features.shape}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå CLIP detailed test failed: {e}")
        return False

def main():
    print("üîç COMPLETE ENVIRONMENT VERIFICATION")
    print("=" * 60)
    
    # Run all tests
    test_basic_setup()
    lib_results = test_core_libraries()
    st_ok = test_sentence_transformers_detailed()
    trt_ok = test_tensorrt()
    triton_ok = test_triton()
    clip_ok = test_clip_detailed()
    
    print("\n" + "=" * 60)
    print("üìã FINAL SUMMARY:")
    print("=" * 60)
    
    # Core requirements
    core_ready = (
        torch.cuda.is_available() and 
        lib_results.get('transformers', False) and
        lib_results.get('clip', False) and
        lib_results.get('faiss', False) and
        lib_results.get('pymongo', False)
    )
    
    # Advanced requirements
    advanced_ready = st_ok and trt_ok and triton_ok
    
    if core_ready and advanced_ready:
        print("üéâ ALL SYSTEMS GO!")
        print("‚úÖ Core ML libraries: Working")
        print("‚úÖ GPU support: Working") 
        print("‚úÖ Sentence transformers: Working")
        print("‚úÖ TensorRT: Working")
        print("‚úÖ Triton client: Working")
        print("\nüöÄ Ready to proceed with MongoDB setup!")
        
    elif core_ready:
        print("‚úÖ CORE SYSTEM READY")
        print("‚úÖ Basic ML pipeline can work")
        print("‚ö†Ô∏è Missing components:")
        if not st_ok:
            print("   - Sentence transformers needs fixing")
        if not trt_ok:
            print("   - TensorRT needs installation")
        if not triton_ok:
            print("   - Triton client needs installation")
        print("\nüìù Can proceed with basic setup, add missing components later")
        
    else:
        print("‚ùå CORE ISSUES DETECTED")
        print("‚ùå Need to fix basic libraries before proceeding")
        
    print("\n" + "=" * 60)

if __name__ == "__main__":
    main()